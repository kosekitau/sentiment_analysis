{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CBET_KimCNN1d.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AsWoCfh-vNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5bdba12-7324-46d8-92d3-205327e12c39"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#gpuの確認\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aisHQGfC_KCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6bfd72c0-e77f-4416-e148-e94ba553e19f"
      },
      "source": [
        "#学習済みの分散表現をロードする\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "english_fasttext_vectors = Vectors(name='drive/My Drive/wiki-news-300d-1M.vec')\n",
        "\n",
        "print(english_fasttext_vectors.dim)\n",
        "print(len(english_fasttext_vectors.itos))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/999994 [00:00<?, ?it/s]Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 998945/999994 [01:39<00:00, 10161.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-dOKi53AIh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2da251eb-0aaa-4cb5-de4e-16c9973102de"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# 以下の記号はスペースに置き換えます（カンマ、ピリオドを除く）。\n",
        "# punctuationとは日本語で句点という意味です\n",
        "print(\"区切り文字：\", string.punctuation)\n",
        "# !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "\n",
        "# 前処理\n",
        "def preprocessing_text(text):\n",
        "    # 改行コードを消去\n",
        "    text = re.sub('<br />', '', text)\n",
        "\n",
        "    # カンマ、ピリオド以外の記号をスペースに置換\n",
        "    for p in string.punctuation:\n",
        "        if (p == \".\") or (p == \",\"):\n",
        "            continue\n",
        "        else:\n",
        "            text = text.replace(p, \" \")\n",
        "\n",
        "    # ピリオドなどの前後にはスペースを入れておく\n",
        "    text = text.replace(\".\", \" . \")\n",
        "    text = text.replace(\",\", \" , \")\n",
        "    return text\n",
        "\n",
        "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
        "def tokenizer_punctuation(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "\n",
        "# 前処理と分かち書きをまとめた関数を定義\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer_punctuation(text)\n",
        "    return ret\n",
        "\n",
        "\n",
        "# 動作を確認します\n",
        "print(tokenizer_with_preprocessing('I like cats+'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "区切り文字： !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['I', 'like', 'cats']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLUQPhQvAf6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#テキストに処理を行うFieldを定義\n",
        "#fix_lengthはtokenの数\n",
        "TEXT = torchtext.data.Field(sequential=True, use_vocab=True, tokenize=tokenizer_with_preprocessing,\n",
        "                            lower=True, include_lengths=True, batch_first=True, fix_length=40)\n",
        "\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "#pandasでcsvを保存するときに、labelをintでキャストしておかないとエラーでるから注意\n",
        "train_ds, val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
        "    path='drive/My Drive/dataset/CBET/ekman', train='train.csv', validation='val.csv', \n",
        "    test='test.csv', format='csv', fields=[('Text', TEXT), ('Label', LABEL)])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe3UVOMwDpFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6c4ef00-7346-46f2-ee90-0f5cef3d544d"
      },
      "source": [
        "#ボキャブラリを作成する\n",
        "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors)\n",
        "\n",
        "print(len(TEXT.vocab.stoi))\n",
        "\n",
        "batch_size = 64\n",
        "d_model = 300\n",
        "num_filters = [100, 100, 100]\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_unit = 100\n",
        "output_dim = 5\n",
        "dropout_rate = 0.3"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGS_HITaDtIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "12e83175-1e60-4cb4-ffaf-ded22ab7fafa"
      },
      "source": [
        "#データローダを作成\n",
        "train_dl = torchtext.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
        "val_dl = torchtext.data.Iterator(val_ds, batch_size=batch_size, train=False, sort=False)\n",
        "test_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "#テスト\n",
        "batch = next(iter(val_dl))\n",
        "print(len(batch.Text[0][0]))\n",
        "print(batch.Label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n",
            "tensor([4, 0, 2, 0, 2, 4, 4, 3, 1, 4, 2, 2, 1, 2, 2, 2, 1, 2, 3, 3, 0, 3, 4, 1,\n",
            "        1, 3, 0, 2, 4, 1, 2, 2, 2, 3, 1, 2, 4, 3, 3, 4, 0, 0, 1, 3, 0, 3, 4, 0,\n",
            "        0, 1, 2, 2, 1, 1, 4, 3, 2, 3, 3, 3, 2, 4, 4, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mZYxj4_Nwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#分散表現の更新をしない\n",
        "class Embedder_static(nn.Module):\n",
        "  def __init__(self, text_embedding_vectors, dropout_rate):\n",
        "    super().__init__()\n",
        "    #tokenの数と、分散表現の次元数\n",
        "    self.embeddings = nn.Embedding.from_pretrained(\n",
        "        embeddings=text_embedding_vectors, freeze=True)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x).permute(0, 2, 1) #[batch, d_model, length]\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "#分散表現を更新する\n",
        "class Embedder_nonstatic(nn.Module):\n",
        "  def __init__(self, text_embedding_vectors, dropout_rate):\n",
        "    super().__init__()\n",
        "    #tokenの数と、分散表現の次元数\n",
        "    self.embeddings = nn.Embedding.from_pretrained(\n",
        "        embeddings=text_embedding_vectors, freeze=False)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x).permute(0, 2, 1) #[batch, d_model, length]\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class CNN_Kim_Layer(nn.Module):\n",
        "  def __init__(self, d_model, num_filters, filter_sizes, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.convs = nn.ModuleList([nn.Conv1d(d_model, nf, fs) for nf, fs in zip(num_filters, filter_sizes)])\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = [F.relu(conv(x).permute(0, 2, 1).max(1)[0]) for conv in self.convs] #[3, batch, filter_size]\n",
        "    return x\n",
        "\n",
        "  \n",
        "class ClassificationHead(nn.Module):\n",
        "  def __init__(self, num_unit, output_dim, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(num_unit*6, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    nn.init.normal_(self.linear.weight, std=0.02)\n",
        "    nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    # torch.cat(x, 1).shape -> [batch, sum(filter_sizes)]\n",
        "    x1.extend(x2)\n",
        "    x = self.linear(torch.cat(x1, 1)) # [batch, output_dim]\n",
        "    output = self.dropout(x)\n",
        "    return output\n",
        "\n",
        "\n",
        "class CNN_Classification(nn.Module):\n",
        "  def __init__(self, text_embedding_vectors, d_model, num_filters, filter_sizes, num_unit, droutput_dim, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.net1_1 = Embedder_static(text_embedding_vectors, dropout_rate)\n",
        "    self.net1_2 = Embedder_nonstatic(text_embedding_vectors, dropout_rate)\n",
        "    self.net2 = CNN_Kim_Layer(d_model, num_filters, filter_sizes, dropout_rate)\n",
        "    self.net3 = ClassificationHead(num_unit, output_dim, dropout_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1_1 = self.net1_1(x) # [batch_size, ntoken, d_model]\n",
        "    x1_2 = self.net1_2(x)\n",
        "    x2_1 = self.net2(x1_1)\n",
        "    x2_2 = self.net2(x1_2) \n",
        "    out = self.net3(x2_1, x2_2) \n",
        "    return out"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_WmWek5EOXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4244b164-4068-4c40-e218-2b89fe044ae0"
      },
      "source": [
        "#テスト\n",
        "batch = next(iter(train_dl))\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "# モデル構築\n",
        "net = CNN_Classification(TEXT.vocab.vectors, d_model, num_filters, filter_sizes, \n",
        "                         num_unit, output_dim, dropout_rate) \n",
        "\n",
        "# 入出力\n",
        "x = batch.Text[0]\n",
        "x1 = net(x)\n",
        "\n",
        "print(\"入力のテンソルサイズ：\", x.shape)\n",
        "print(\"出力のテンソルサイズ：\", x1.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 torch.Size([64, 100])\n",
            "torch.Size([64, 5])\n",
            "入力のテンソルサイズ： torch.Size([64, 40])\n",
            "出力のテンソルサイズ： torch.Size([64, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwUyntMHEUwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}